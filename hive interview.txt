Q1. What is the definition of Hive? What is the present version of Hive?

Ans: Apache Hive is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale.
     A data warehouse provides a central store of information that can easily be analyzed to make informed, data driven 
     decisions. Hive allows users to read, write, and manage petabytes of data using similar to SQL. The present version of Hive is 0.13.



Q2. Is Hive suitable to be used for OLTP systems? Why?

Ans: Hive doesn’t support OLTP. Hive supports Online Analytical Processing (OLAP), but not Online Transaction Processing (OLTP).
     Hive does not provide insert and update at row level. So it is not suitable for OLTP system.



Q3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason

Ans: RDBMS is a such type of database management system which is specifically designed for relational databases. while Hive is a data warehouse 
     software system that provides data query and analysis. Hive gives an interface like SQL to query data stored in various databases and 
     file systems that integrate with Hadoop.  Hive supports ACID transactions on tables that store ORC file format. limitations of using Hive ACID transactions.

     To support ACID, Hive tables should be created with TRANSACTIONAL table property.
     Currently, Hive supports ACID transactions on tables that store ORC file format.
     Enable ACID support by setting transaction manager to DbTxnManager
     Transaction tables cannot be accessed from the non-ACID Transaction Manager (DummyTxnManager) session.
     External tables cannot be created to support ACID since the changes on external tables are beyond Hive control.
     LOAD is not supported on ACID transactional Tables. hence use INSERT INTO.
     On the Transactional session, all operations are auto commit as BEGIN, COMMIT, and ROLLBACK are not yet supported.


Q4. Explain the hive architecture and the different components of a Hive architecture?

Ans: the architecture of Apache Hive and its major components. The major components of Apache Hive are:

     Hive Client
     Hive Services
     Processing and Resource Management
     Distributed Storage

   Hive clients are categorized into three types:

   1. Thrift Clients
   The Hive server is based on Apache Thrift so that it can serve the request from a thrift client.

   2. JDBC client
   Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server.

   3. ODBC client
   Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate 
   with the Hive Server.

   To perform all queries, Hive provides various services like the Hive server2, Beeline, etc. The various services offered by Hive are:

   1. Beeline
   The Beeline is a command shell supported by HiveServer2, where the user can submit its queries and command to the system. It is a JDBC client 
   that is based on SQLLINE CLI (pure Java-console-based utility for connecting with relational databases and executing SQL queries).

   2. Hive Server 2
    HiveServer2 is the successor of HiveServer1. HiveServer2 enables clients to execute queries against the Hive. It allows multiple clients to
   submit requests to Hive and retrieve the final results. It is basically designed to provide the best support for open API clients like JDBC and ODBC.


    It does not handle concurrent requests from more than one client due to which it was replaced by HiveServer2.

   3. Hive Driver
   The Hive driver receives the HiveQL statements submitted by the user through the command shell. It creates the session handles for the query and sends the query to the compiler.

   4. Hive Compiler
   Hive compiler parses the query. It performs semantic analysis and type-checking on the different query blocks and query expressions by using the metadata 
   stored in metastore and generates an execution plan.

   The execution plan created by the compiler is the DAG(Directed Acyclic Graph), where each stage is a map/reduce job, operation on HDFS, a metadata operation.
 
   5. Optimizer
   Optimizer performs the transformation operations on the execution plan and splits the task to improve efficiency and scalability.

   6. Execution Engine
   Execution engine, after the compilation and optimization steps, executes the execution plan created by the compiler in order of their dependencies using Hadoop.

   7. Metastore
   Metastore is a central repository that stores the metadata information about the structure of tables and partitions, including column and column type information.

   It also stores information of serializer and deserializer, required for the read/write operation, and HDFS files where data is stored. This metastore is 
   generally a relational database.

   Metastore provides a Thrift interface for querying and manipulating Hive metadata.

   We can configure metastore in any of the two modes:

   Remote: In remote mode, metastore is a Thrift service and is useful for non-Java applications.
   Embedded: In embedded mode, the client can directly interact with the metastore using JDBC.
   8. HCatalog
   HCatalog is the table and storage management layer for Hadoop. It enables users with different data processing tools such as Pig, MapReduce, etc. to easily read and write data on the grid.

   It is built on the top of Hive metastore and exposes the tabular data of Hive metastore to other data processing tools.

   9. WebHCat
   WebHCat is the REST API for HCatalog. It is an HTTP interface to perform Hive metadata operations. It provides a service to the user for running Hadoop MapReduce (or YARN), Pig, Hive jobs.

   Hive internally uses a MapReduce framework as a defacto engine for executing the queries.

   MapReduce is a software framework for writing those applications that process a massive amount of data in parallel on the large clusters of commodity hardware.
   MapReduce job works by splitting data into chunks, which are processed by map-reduce tasks.


Q5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?

Ans: Hive query processor convert graph of MapReduce jobs with the execution time framework.  So that the jobs can be executed in the order of dependencies.
     The components of a Hive query processor include,

    Logical Plan Generation
    Physical Plan Generation
    Execution Engine
    Operators
    UDF’s and UDAF’s
    Optimizer
    Parser
    Semantic Analyzer
    Type Checking


Q6. What are the three different modes in which we can operate Hive?

Ans: We can run Hive in following modes:

     Local mode: In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
    
     Distributed Mode: In this mode, Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on 
     different machines in this mode.
 
     Pseudo-distributed Mode: This is the mode used by developers to test the code before deploying to production. In this mode, all the daemons run on 
     same virtual machine. With this mode, we can quickly write scripts and test on limited data sets.


Q7. Features and Limitations of Hive.

Ans: Features of Apache Hive

    The Various key-features of Apache Hive are:

    1. Open-source: Apache Hive is an open-source tool. We can use it free of cost.
 
    2. Query large datasets: Hive can query and manage huge datasets stored in Hadoop Distributed File System.

    3. Multiple-users: Multiple users can query the data using Hive Query Language simultaneously.

    4. Backward compatible: Apache Hive perfectly fits the low level interface requirement of Apache Hadoop.

    5. Partitioning and Bucketing: Apache Hive supports partitioning and bucketing of data at the table level to improve performance.

    6. File-formats: Hive provides support for various file formats such as textFile, ORC, Avro Files, SequenceFile, Parquet, RCFile, LZO Compression etc.

    7. Hive Query Language: Hive uses Hive Query Language which is similar to SQL. We do not require any knowledge of programming languages to work with Hive. Only the knowledge of basic SQL query is enough to work with Hive.

    8. Built-In function: Hive provides various Built-In functions.

    9. User-Defined Functions: It also provides support for User-Defined Functions for the tasks like data cleansing and filtering. We can define UDFs according to our requirements

    10. External Table: Apache Hive supports external tables. This allows us to process data without actually storing data in HDFS.

    11. Fast: Hive is a fast, scalable, extensible tool and uses familiar concepts.

    12. Warehouse: Apache Hive is a distributed data warehouse tool.

    13. Table Structure: Table structure in Hive is similar to table structure in RDBMS.

    14. ETL support: Hive supports ETL operations. Hive is an effective ETL tool.

    15. Storage: Hive allows us to access files stored in HDFS and other similar data storage systems such as HBase.


     Some of the limitations of Apache Hive are:

     1. Hive is not designed for the OLTP (Online transaction processing). We can use it for OLAP.

     2. It does not offer real-time queries.
 
     3. It provides limited subquery support.

     4. Latency of Hive is generally very high.


Q8. How to create a Database in HIVE?

Ans: create a database if not exists first_db;  


Q9. How to create a table in HIVE?

Ans: create table first_table ;



Q10. What do you mean by describe and describe extended and describe
     formatted with respect to database and table

Ans: The DESCRIBE statement displays metadata about a table, such as the column names and their data types. 
     The DESCRIBE FORMATTED variation displays additional information, in a format familiar to users of Apache Hive. 
     The extra information includes low-level details such as whether the table is internal or external, when it was created, the file format, the 
     location of the data in HDFS, whether the object is a table or a view, and (for views) the text of the query from the view definition.


Q11. How to skip header rows from a table in Hive?

Ans: CREATE EXTERNAL TABLE Test ( 
     RecordId int, 
     FirstName string, 
     LastName string 
      ) 
     row format delimited
     fields terminated by ',' 
     tblpropertied(“skip.header.line.count"=”1”);



Q12. What is a hive operator? What are the different types of hive operators?

Ans: Hive operators are used for mathematical operations on operands. It returns specific value as per the logic applied.
   
     there are four types of operators in Hive:

     Relational Operators
     Arithmetic Operators
     Logical Operators
     Complex Operators


Q13. Explain about the Hive Built-In Functions

Ans: Basically, to perform several operations there are some functions available. Similarly, in Hive also there are some built-in functions available. Such as Hive Collection Functions, Hive Date Functions, 
     Hive Mathematical Functions, Hive Conditional Functions and Hive String Functions.


Q14. Write hive DDL and DML commands.

Ans: Hive DDL commands are the statements used for defining and changing the structure of a table or database in Hive. 
     It is used to build or modify the tables and other objects in the database.

    The several types of Hive DDL commands are:

    CREATE
    SHOW
    DESCRIBE
    USE
    DROP
    ALTER
    TRUNCATE

    The various Hive DML commands are:

     LOAD
    SELECT
    INSERT
    DELETE
    UPDATE
    EXPORT
    IMPORT


Q15 Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.

Ans: Sortby: The SORT by clause sorts the data per reducer. As a result, if we have N number of reducers, we will have N number of sorted files in the output. 
     These files can have overlapping data ranges. 

     orderby: ORDER BY clause orders the data globally. Because it ensures the global ordering of the data, all the data need to be passed from a single reducer 
     only. As a result, the order by clause outputs one single file only.

     distribute by : DISTRIBUTE BY clause is used to distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the 
     same reducer. So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries. However, the DISTRIBUTE BY 
     clause does not sort the data either at the reducer level or globally. Also, the same key values might not be placed next to each other in the output dataset.

     cluster by : CLUSTER BY clause is a combination of DISTRIBUTE BY and SORT BY clauses together. That means the output of the CLUSTER BY clause is equivalent to the
     output of DISTRIBUTE BY + SORT BY clauses. 



Q16. Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?

Ans: Internal Table. 
     Hive owns the data for the internal tables.

    It is the default table in Hive. When the user creates a table in Hive without specifying it as external, then by default, an internal table gets created in a specific location in HDFS.
    By default, an internal table will be created in a folder path similar to /user/hive/warehouse directory of HDFS. We can override the default location by the location property during table creation.
    If we drop the managed table or partition, the table data and the metadata associated with that table will be deleted from the HDFS.

    Hive External Table
    Hive does not manage the data of the External table.

    We create an external table for external use as when we want to use the data outside the Hive.External tables are stored outside the warehouse directory. 
    They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes. Whenever we drop the external table, then only the metadata
    associated with the table will get deleted, the table data remains untouched by Hive. We can create the external table by specifying the EXTERNAL keyword in the Hive create table statement.



Q17. Where does the data of a Hive table get stored?

Ans: Hive stores data at the HDFS location /user/hive/warehouse folder if not specified a folder using the LOCATION clause while creating a table. Hive is a data
     warehouse database for Hadoop, all database and table data files are stored at HDFS location /user/hive/warehouse by default, you can also store the Hive data
     warehouse files either in a custom location on HDFS


Q18. Is it possible to change the default location of a managed table?

Ans: by using the LOCATION keyword, we can change the default location of Managed tables while creating the managed table in Hive. However, to do so, the user needs to specify the storage path of the 
     managed table as the value to the LOCATION keyword, that will help to change the default location of a managed table.


Q19. What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?

Ans: Basically, the central repository of Apache Hive metadata is what we call Hive Metastore. As its job, it keeps the metadata for Hive tables (like their schema and location) as well as partitions in a relational database.
     Derby is the default database for the embedded metastore.


Q20. Why does Hive not store metadata information in HDFS?

Ans: Hive stores metadata information in the metastore using RDBMS instead of HDFS. The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.


Q21. What is a partition in Hive? And Why do we perform partitioning in Hive?

Ans: Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.
     The advantage of partitioning is that since the data is stored in slices, the query response time becomes faster.


Q22. What is the difference between dynamic partitioning and static partitioning?

Ans: In Static Partitioning, we have to manually decide how many partitions tables will have and also value for those partitions. Consider we have employ table and we want to partition it based on department name. 
    There are a limited number of departments, hence a limited number of partitions.      
    Dynamic partitions provide us with flexibility and create partitions automatically depending on the data that we are inserting into the table.


Q23. How do you check if a particular partition exists?

Ans: SHOW PARTITIONS table_name 

     PARTITION(partitioned_column=’partition_value’)


Q24. How can you stop a partition form being queried?

Ans: By using the ENABLE OFFLINE clause with ALTER TABLE atatement.


Q25. Why do we need buckets? How Hive distributes the rows into buckets?

Ans: The bucketing in Hive is a data organizing technique. It is similar to partitioning in Hive with an added functionality that it divides large datasets into more manageable parts known as buckets. 
     So, we can use bucketing in Hive when the implementation of partitioning becomes difficult. 
     The concept of bucketing is based on the hashing technique. Here, modules of current column value and the number of required buckets is calculated (let say, F(x) % 3). Now, based on the resulted value,
     the data is stored into the corresponding bucket.


Q26. In Hive, how can you enable buckets?

Ans: By using this command below bucket can be enabled;

     set.hive.enforce.bucketing=true;


Q27. How does bucketing help in the faster execution of queries?

Ans: Bucketing in hive is the concept of breaking data down into ranges, which are known as buckets, to give extra structure to the data so it may be used for more efficient queries.
     The range for a bucket is determined by the hash value of one or more columns in the dataset


Q28. How to optimise Hive Performance? Explain in very detail

Ans: In the hive, numerous optimizations can be implemented.
     
     1) Partitioning and Bucketing
        The organization of data is critical to query performance. Whether you employ a star schema or a de-normalized (preferred) data warehouse depends on your preferences.
        Partitioning and bucketing aid in query performance optimization.  Hive reads the entire directory without splitting the data. Furthermore, the query filters
        are applied to it. Because every data must be read, this is a time-consuming and costly process. Users must also often filter data based on certain column values

     2) Using appropriate file formats
        If we utilize an appropriate file format based on the data, it will significantly improve the speed of our queries. ORC (Optimized Row Columnar) is a good choice for boosting query performance. 
        This means we can store data more efficiently than other file formats. ORC, to be more precise, decreases the amount of the original data by up to 75%

     3) Compression
        Internally, compression techniques limit the quantity of data transfers between mappers and reducers by considerably reducing the intermediate data volume. All of this usually takes place through the internet. 
        The mapper and reducer outputs can each be compressed separately.

     4) De-normalizing data
        Normalization is a typical procedure for modeling your data tables with certain principles in order to deal with data redundancy and anomalies. To put it another way, normalizing your data sets results in the creation
        of numerous relational tables that may be combined at run time to give the desired results.

     5) Parallel execution
        Hadoop can run MapReduce processes in parallel, and numerous Hive queries leverage this parallelism automatically. Single, sophisticated Hive queries, on the other hand, are frequently converted into many MapReduce tasks, 
        which are then run in the order in which they were created.

     6) Vectorization
        Hive uses vectorization to handle a batch of rows at once rather than one row at a time. A column vector, which is commonly an array of primitive types, is included in each batch. It is 
        accomplished by doing them in batches of 1024 rows at a time, rather than one row at a time.

     7) Optimizing Join
        If the table on the other side of the join is small enough to fit in memory, map joins are efficient. When set to "true," Hive offers an option called hive.auto.convert.join, which indicates that 
        Hive tries to map join automatically. When using this parameter, make sure the Hive environment's auto-convert is turned on.


Q29.  What is the use of Hcatalog?

Ans: HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications. 


Q30. Explain about the different types of join in Hive.

Ans: it is more or less similar to SQL JOIN. Also, we use it to combine rows from multiple tables.
     Inner join in Hive
     Left Outer Join in Hive 
     Right Outer Join in Hive
     Full Outer Join in Hive 
     

Q31. Is it possible to create a Cartesian join between 2 tables, using Hive?

Ans: yes we can  create a Cartesian join between 2 tables


Q32. Explain the SMB Join in Hive?

Ans: SMB is a join performed on bucket tables that have the same sorted, bucket, and join condition columns. It reads data from both bucket tables and performs common
     joins (map and reduce triggered) on the bucket tables.


Q33 What is the difference between order by and sort by which one we should use?

Ans: The difference between "order by" and "sort by" is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer. If there are more than one reducer,
     "sort by" may give partially ordered final results.


Q34. What is the usefulness of the DISTRIBUTED BY clause in Hive?

Ans: DISTRIBUTE BY controls how map output is divided among reducers. By default, MapReduce computes a hash on the keys output by mappers and tries to evenly distribute the key-value pairs among the available
     reducers using the hash values. Say we want the data for each value in a column to be captured together. We can use DISTRIBUTE BY to ensure that the records for each go to the same reducer. DISTRIBUTE BY works
     similar to GROUP BY in the sense that it controls how reducers receive rows for processing, Note that Hive requires that the DISTRIBUTE BY clause come before the SORT BY clause if it's in same query .


Q35. How does data transfer happen from HDFS to Hive?

Ans: To query data in HDFS in Hive, you apply a schema to the data and then store data in ORC format. Incrementally update the imported data. Updating imported tables involves importing incremental changes 
     made to the original table using Sqoop and then merging changes with the tables imported into Hive.


Q36 .Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?

Ans: it creates the local metastore, while we run the hive in embedded mode. Also, it looks whether metastore already exist or not before creating the metastore.


Q37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?

Ans: The command:  ‘SET hive.enforce.bucketing=true;’ allows one to have the correct number of reducer while using ‘CLUSTER BY’ clause for bucketing a column. In case it’s not done, one may find the number
     of files that will be generated in the table directory to be not equal to the number of buckets. As an alternative, one may also set the number of reducer equal to the number of buckets by using
     set mapred.reduce.task = num_bucket.


Q38. Can a table be renamed in Hive?

Ans: yes we can , ALTER TABLE name RENAME TO new_name


Q39. Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)

Ans: ALTER TABLE table_name

     CHANGE COLUMN new_col  INT

     BEFORE x_col


Q40  What is serde operation in HIVE?

Ans: A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats. 


Q41. Explain how Hive Deserializes and serialises the data?

Ans: Mapreduce uses input format to get line in a row then deserializationm takes place  line row converted into coloumn using Serde loibrary 
     hive use Serde to convert coloumns into line of row and serialization tale place lin eof row stored into HDFS by using mapreduce in output format



Q42. Write the name of the built-in serde in hive.

Ans: MetadataTypedColumnsetSerDe and LazySimpleSerDe



Q43. What is the need of custom Serde?

Ans: Despite Hive SerDe users want to write a Deserializer in most cases. It is because users just want to read their own data format instead of writing to it
     By using the configuration parameter ‘regex’, the RegexDeserializer will deserialize the data, and possibly a list of column names 



Q44. Can you write the name of a complex data type(collection data types) in Hive?

Ans: There are three complex types in hive,

    arrays: It is an ordered collection of elements.The elements in the array must be of the same type.

    map: It is an unordered collection of key-value pairs.Keys must be of primitive types.Values can be of any type.

    struct: It is a collection of elements of different types.



Q45. Can hive queries be executed from script files? How?

Ans: We can execute Hive queries from the script files using the source command.
     For example −
     Hive> source /path/to/file/file_with_query.hql



Q46. What are the default record and field delimiter used for hive text files?

Ans: The default record delimiter is − \n
      And the filed delimiters are − \001,\002,\003


Q47. How do you list all databases in Hive whose name starts with s?

Ans: SHOW (DATABASES|SCHEMAS) [LIKE identifier_with_wildcards];
     Shoe databases like 'S*' ;


Q48. What is the difference between LIKE and RLIKE operators in Hive?

Ans: LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text. 
     RLIKE (Right-Like) is a special function in Hive where if any substring of A matches with B then it evaluates to true. It also obeys Java regular expression pattern.


Q49. How to change the column data type in Hive?

Ans: ALTER TABLE table_name CHANGE column_name column_name new_datatype;


Q50. How will you convert the string ’51.2’ to a float value in the particular column?

Ans: Hive CAST(from_datatype as to_datatype) function is used to convert from one data type to another 
     cast('51.2' to float )


Q51. What will be the result when you cast ‘abc’ (string) as INT?

Ans: cast('abc' to string )


Q52. What does the following query do?
    a. INSERT OVERWRITE TABLE employees
    b. PARTITION (country, state)
    c. SELECT ..., se.cnty, se.st
    d. FROM staged_employees se;

Ans:  a. INSERT OVERWRITE TABLE employees :- INSERT OVERWRITE is used to replace any existing data in the employee insert with the new rows 
       b. PARTITION (country, state):- PARTITIONED BY is used to create a partition table here country and within country state will be created
      c. SELECT ..., se.cnty, se.st :- here coloumn cnty and st will br selected from table se 
      d. FROM staged_employees se :- here the table staged_employees will be selected and it will be alias as se 



Q53. Write a query where you can overwrite data in a new table from the existing table

Ans: Insert overwrite table new table select * from existing table ;




Q54. What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.

Ans: The maximum size of a string data type supported by Hive is 2 GB. Hive supports the text file format by default, and it also supports the binary
     format sequence files, ORC files, Avro data files, and Parquet files.



Q55. What File Formats and Applications Does Hive Support?

Ans: Hive supports the text file format by default, and it also supports the binary format sequence files, ORC files, Avro data files, and Parquet files.



Q56. How do ORC format tables help Hive to enhance its performance?

Ans: Using the ORC format leads to a reduction in the size of the data stored, as this file format has high compression ratios. As the data size is reduced, 
     the time to read and write the data is also reduced.


Q57. How can Hive avoid mapreduce while processing the query?

Ans: conversion property can (FETCH task) minimize latency of mapreduce overhead. When queried SELECT, FILTER, LIMIT queries, this property skip 
     mapreduce and using FETCH task. As a result Hive can execute query without run mapreduce task.

 
Q58. What is view and indexing in hive?

Ans: Apache Hive View is a searchable object in a database which we can define by the query. On defining indexing in Hive we can say these are pointers to 
     particular column name of a table.


Q59. Can the name of a view be the same as the name of a hive table?

Ans: The name of a view must be unique, and it cannot be the same as any table or database or view's name.



Q60. What types of costs are associated in creating indexes on hive tables?

Ans:  Basically, there is a processing cost in arranging the values of the column on which index is created since Indexes occupies.



Q61.Give the command to see the indexes on a table.

Ans: to see the index for a specific table use SHOW INDEX: SHOW INDEX FROM yourtable; 



Q62. Explain the process to access subdirectories recursively in Hive queries.

Ans: TBLPROPERTIES ("hive.input.dir.recursive" = "TRUE", 
    "hive.mapred.supports.subdirectories" = "TRUE",
    "hive.supports.subdirectories" = "TRUE", 
    "mapred.input.dir.recursive" = "TRUE");



Q63. If you run a select * query in Hive, why doesn't it run MapReduce?

Ans: Hive requires a map-reduce job since it needs to extract the 'column' from each row by parsing it from the file it loads. while in * no filtering is required



Q64. What are the uses of Hive Explode?

Ans: Explode in Hive is used to convert complex data types into desired table formats. explode
     UDTF basically emits all the elements in an array into multiple rows.



Q65. What is the available mechanism for connecting applications when we run Hive as a server?

Ans: Thrift Client: Using Thrift, we can call Hive commands from various programming languages, such as C++, PHP, Java, Python, and Ruby.



Q66. Can the default location of a managed table be changed in Hive?

Ans: by using the LOCATION keyword, we can change the default location of Managed tables while creating the managed table in Hive. However, to do so, the user
     needs to specify the storage path of the managed table as the value to the LOCATION keyword, that will help to change the default location of a managed table.



Q67. What is the Hive ObjectInspector function?

ANs: Hive ObjectInspector is a group of flexible APIs to inspect value in different data representation, and developers can extend those API as needed, so technically,
     object inspector supports arbitrary data type in java.



Q68. What is UDF in Hive?

Ans: we can use two different interfaces for writing Apache Hive User Defined Functions.

    Simple API
    Complex API
    As long as our function reads and returns primitive types, we can use the simple API (org.apache.hadoop.hive.ql.exec.UDF).



Q69. Write a query to extract data from hdfs to hive

Ans: load data inpath '/user/warehouse/table' into table newtable ;



Q70. What is TextInputFormat and SequenceFileInputFormat in hive

Ans: Hive Text file format is a default storage format to load data from comma-separated values (CSV), tab-delimited, space-delimited,
     or text files that delimited by other special characters.
     SequenceFileInputFormat in Hadoop is an InputFormat which reads sequence files. Sequence files are binary files that stores sequences of binary key-value pairs.
     Sequence files are block-compressed and provide direct serialization and deserialization of several arbitrary data types (not just text).


Q71. How can you prevent a large job from running for a long time in a hive?

Ans: This can be achieved by setting the MapReduce jobs to execute in strict mode set hive.


Q72. When do we use explode in Hive?

Ans: Lateral View Explode is another function in Hive that is used to split a column, but instead of creating multiple rows,
     it creates multiple columns. This function is beneficial when working with maps.


Q73. Can Hive process any type of data formats? Why? Explain in very detail

Ans: Hive supports four file formats those are TEXTFILE, SEQUENCEFILE, ORC and RCFILE (Record Columnar File). For single user metadata storage,
     Hive uses derby database and for multiple user Metadata or shared Metadata case Hive uses MYSQL


Q74. Whenever we run a Hive query, a new metastore_db is created. Why?

Ans: Whenever you run the hive in embedded mode, it creates the local metastore. And before creating the metastore it looks whether 
     metastore already exist or not. This property is defined in configuration file hive-site.xml. Property is “javax.jdo.option.ConnectionURL” 
     with default value “jdbc:derby:;databaseName=metastore_db;create=true”. So tochange the behavior of the location to an absolute path so that 
     from that location the metastore can be used.


Q75. Can we change the data type of a column in a hive table? Write a complete query.

Ans: yes, we change the data type of a column in a hive table 
     ALTER TABLE table_name CHANGE column_name column_name new_datatype;


Q76. While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?

Ans: but not using 'load data Local inpath' keyword 



Q77. What is the precedence order in Hive configuration?

ANs: We are using a precedence hierarchy for setting properties: The SET command in Hive. The command-line –hiveconf option



Q78. Which interface is used for accessing the Hive metastore?

Ans: Which interface is used for accessing the Hive metastore? Ans. WebHCat API web interface can be used for Hive commands. It is a 
     REST API that allows applications to make HTTP requests to access the Hive metastore (HCatalog DDL).


Q79. Is it possible to compress json in the Hive external table ?

Ans: yes it is possible .


Q80. What is the difference between local and remote metastores?

Ans: local metastore:- Here metastore service still runs in the same JVM as Hive but it connects to a database running in a separate process either
     on same machine or on a remote machine. Remote Metastore:- Metastore runs in its own separate JVM not on hive service JVM.


Q81.What is the purpose of archiving tables in Hive?

Ans: You can use Hadoop archiving to reduce the number of hdfs files in the Hive table partition. Hive has built in functions to convert Hive table
     partition into Hadoop Archive (HAR). HAR does not compress the files, it is analogous to the Linux tar command.


Q82. What is DBPROPERTY in Hive?

Ans: The DB properties are nothing but mentioning the details about the database created by the user. Suppose the name of the user, 
     the type of the database and the tables it has, the date on which the database is created etc. This makes the other user easy the recognize the 
     database and use it according to the requirement.


Q83. Differentiate between local mode and MapReduce mode in Hive.

Ans: Both MapReduce mode and local mode seem same to the user but the difference is the way they execute. Local mode: In this mode, Pig script 
     runs on a Single machine without the need of Hadoop cluster or hdfs. Local mode is used for development purpose to see how the script would behave
     in an actual environment